<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>Responsive website</title>
</head>
<body>
    <header>
    <div class="menu-section">
        <img src="../Homework/images/menu.png" class="menu">
    </div>

    <div class="logo">
        <img src="../Homework/images/ufo.png" class="ufo">
        <h1 class="bert">BERT</h1>
    </div>

    <div class="right-icons">
        <img src="../Homework/images/play.png" class="play">
        <img src="../Homework/images/share.png" class="share">
        <img src="../Homework/images/en.png" class="en">
    </div>
    </header>

    <main>
    <div class="div-main">
        <h1 class="main">HELLO</h1>
        <p>Bidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learned by self-supervised learning to represent text as a sequence of vectors. It had the transformer encoder architecture It was notable for its dramatic improvement over previous state of the art models, and as an early example of large language model. As of 2020, BERT was a ubiquitous baseline in natural language processing (NLP) experiments.</p>
    </div>
    </main>
    <footer>
        &copy; Made By Gabriel
    </footer>
</body>
</html>